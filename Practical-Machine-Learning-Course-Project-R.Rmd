---

title: "Mohau-Mateyisi-project-report"
author: "Mohau Mateyisi"
date: "13 January 2019"
output: html_document
---

Presented In this notebook, is a summary of steps on model development as summarised in the Report.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Practical Machine Learning project




```{r Data sets}
training = read.csv("D:/Machine Learning/pml-training.csv")
testing  = read.csv("D:/Machine Learning/pml-testing.csv")

str(training)
```

```{r}
table(training$classe)

```
The data contains 19622 obs. of  160 variables. About 22 variable are factors while the rest are numeric. Some variables have missing values dentoed "" while other are NAs. The predictor "classe" variable rate the observables according to factors A to

```{r}

hist(as.numeric(training$classe), xaxt="n")
axis(1, at=seq(1, 5, by=1), labels=c("A", "B", "C", "D", "E"))


```

Most of the observations fall in class A with class B and E having almost
the same frequency while class C and D have almost equal lowest frequency.

## Initial feature Selection

The data has columns with missing values and NAs. First we clean the data to get rid of those columns. We start by getting rid of columns with NAs and missing values for both training and test input data.

```{r Without NA and missing values}

trainOr<-training[ , colSums(is.na(training)) == 0]
trainOr<-trainOr[ , colSums(trainOr == "") == 0]

testOr<-testing[ , colSums(is.na(testing)) == 0]
testOr<-testOr[ , colSums( testOr== "") == 0]

dim(trainOr);dim(testOr)
str(trainOr)
```
This leaves us with 60 variables. 

## Slitting the original traing data into testing and Validation data

To select the validation data set from the training data we randomise the samples
using 75 percent of the data for training and 25 percent for validation( denoted testingVL).

```{r Validation data}
library(caret)

traindf<-createDataPartition(trainOr$classe, p=0.75,list = FALSE)
trainingDt<-trainOr[traindf,]
testingVl<-trainOr[-traindf,]

dim(trainingDt);dim(testingVl)

```


# Picking features from the training set.

Next we get rid of six variables related to time time and window factor variables as well as all the variables that are highly correlated with other variables (at 0.3 correlation coefficient cutoff).

```{r Variable Importance}
library(ElemStatLearn)
library("rpart")
library(gbm)

set.seed(33833)

# Reducing factor Variables not needed for training and testing

trainingRD <-trainingDt[-c(1:6)]
testingVlRD<-testingVl[-c(1:6)]
testOrRD <-testOr[-c(1:6)]


#reducing correlating among variables 
df_cor = cor(trainingRD[-c(54)])
hc = findCorrelation(df_cor, cutoff=0.3) # putt any value as a "cutoff" 
hc = sort(hc)
training_CorRD = trainingRD[,-c(hc)]     # correlation reduced

```

```{r}

RFmod <- train( classe ~ ., data=training_CorRD, method="rf")
#varImp(RFmod)

```

```{r}

RFmod

```


#Setting of prediction functions

Here we simplify the model by leaving out variables of low importance for simplicity in order to obtain faster fit and prediction.

```{r}
RFmod1 <- train(classe ~ num_window +roll_forearm+roll_dumbbell+gyros_belt_z+accel_forearm_z, data=training_CorRD, method="rf")

```

```{r}
RFmod1
```
Now we can test both the random forest on the validation
dataset.

```{r }
predRFmod1 <- predict(RFmod1, testingVlRD)
```

```{r}
table(predRFmod1,testingVlRD$classe)
```

```{r}
confusionMatrix(predRFmod1, testingVlRD$classe)$overall[1]
```


As seen above, our model was approx. 99% accurate. To improve on this accuaracy, we develop a boosted trees ("gbm") model.


```{ }
GBMmod <- train(classe ~ ., data=training_CorRD, method="gbm")
varImp(GBMmod)
```
To four important variable are the same as in random forest models.


This leads to a comparable Model Simplification 
```{r }
GBMmod1 <- train(classe ~num_window +roll_forearm+gyros_belt_z+roll_dumbbell+accel_forearm_z, data=training_CorRD, method="gbm")
```

```{r}

GBMmod1

```

Now we can test simplified boot strapping models on the validation
dataset.

```{r }

predGBMmod1<- predict(GBMmod1, testingVlRD)

```

```{r}
table(predGBMmod1,testingVlRD$classe)
```

```{r }

confusionMatrix(predGBMmod1, testingVlRD$classe)$overall[1]

```
The overall accuracy of the boot strapping model is sligthly lower than that of 
random model. Next we stuck the two model prodictions and developed a hydrid of the two models and test it on the validation data set.

```{r }

PredStuck <- data.frame(predRFmod1, predGBMmod1, classe=testingVlRD$classe)
Stuckpredmod <- train(classe ~., data=PredStuck, method="rf")
predAllmod <- predict(Stuckpredmod, testingVlRD)

```

```{r }
Stuckpredmod
```



#Estimation of out of Sampel error from the validation set. 

```{r }
table(predAllmod, testingVlRD$classe)
confusionMatrix(predAllmod, testingVlRD$classe)$overall[1]

```
The model build by combining the two has a much lower error rate than the two independent models.

It seems reasonable to make use of hybrid model for prediction on the test data set, However the random forest "rf" and the boot strapping  ("gbm") models have a satisfactorily low effor rate such that is no siganificant gain in using the hybrid model.

```{r}

#prediction with boasted trees

PredTestingOr <- predict(GBMmod1, testOrRD)

```

```{r}
#prediction with random forest
PredTestingOr1 <- predict(RFmod1, testOrRD)

```

In summary Both prediction methods yield the same predictions:

```{r}
PredTestingOr1==PredTestingOr1;PredTestingOr1
```

```{r}
table(PredTestingOr1)
table(PredTestingOr)

```

The frequencies distribution is more skeyed towards classes A and B as it was the case with the training data classes and lowest for classes C and D which turn to have an equal value of unity.

```{r}

hist(as.numeric(PredTestingOr1), xaxt="n")
axis(1, at=seq(1, 5, by=1), labels=c("A", "B", "C", "D", "E"))


```
